{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IFN-646 Project: Images Classification of Breast Cancer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description\n",
    "\n",
    "In this file, we will perform how we run the classifiers to classify which case considered as **BENIGN** or **MALIGNANT**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning and Wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section of code, we will do the Data cleaning and Wrangling including do the stratified sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Library needed\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import scipy\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image, ImageOps\n",
    "import cv2\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorboard import notebook\n",
    "from tensorflow.keras.preprocessing.image import Iterator\n",
    "from tensorflow.keras.layers import Input, Dense, Conv2D, MaxPooling2D, UpSampling2D, concatenate, BatchNormalization, SpatialDropout2D, Activation\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import regularizers\n",
    "import tensorflow.keras.backend as K\n",
    "tf.keras.backend.clear_session()\n",
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to set the subject ID for calc and mass dataset\n",
    "def generate_full_mask(name, file):\n",
    "    df_full = pd.read_csv(file)\n",
    "    df_full[['assessment', 'abnormality id']] = df_full[['assessment', 'abnormality id']].astype(str)\n",
    "    df_full['Subject ID'] = name+df_full['patient_id']+'_'+df_full['left or right breast']+'_'+df_full['image view']\n",
    "    df_mask = pd.read_csv(file)\n",
    "    df_mask[['assessment', 'abnormality id']] = df_mask[['assessment', 'abnormality id']].astype(str)\n",
    "    df_mask['Subject ID'] = name+df_mask['patient_id']+'_'+df_mask['left or right breast']+'_'+df_mask['image view']+'_'+df_mask['abnormality id']\n",
    "    df_full = df_full.drop_duplicates(subset=['Subject ID'], keep='first')\n",
    "    df_mask = df_mask.drop_duplicates(subset=['Subject ID'], keep='first')\n",
    "    return df_full, df_mask\n",
    "\n",
    "#Function to get the pathfile\n",
    "def get_name(directory):\n",
    "\n",
    "    names = []\n",
    "\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for filename in files:\n",
    "            _, ext = os.path.splitext(filename)\n",
    "            if ext in ['.png']:\n",
    "                names.append(os.path.join(root, filename))\n",
    "\n",
    "    return names\n",
    "\n",
    "def merge_path(data1, data2, tpe):\n",
    "    #Select necessary column to match with metadata from calc and mass dataset\n",
    "    data1 = data1.iloc[:,[8,9,14]]\n",
    "    data2 = data2.iloc[:,[8,9,14]]\n",
    "    merge1 = pd.merge(data1, result, on='Subject ID', how='inner')\n",
    "    merge2 = pd.merge(data2, result, on='Subject ID', how='inner')\n",
    "    #Only select the mask file, which start with 1-2.dcm (Thus keep the last)\n",
    "    merge2 = merge2.drop_duplicates(subset=['Subject ID'], keep='last')\n",
    "    #Re-index the dataframe\n",
    "    merge1 = merge1.reindex(columns=['Subject ID','assessment','pathology','Match','Pathfile'])\n",
    "    merge2 = merge2.reindex(columns=['Subject ID','assessment','pathology','Match','Pathfile'])\n",
    "    #Change the Benign Without Callback to Benign\n",
    "    merge1['pathology'] = merge1['pathology'].replace(['BENIGN_WITHOUT_CALLBACK'],'BENIGN')\n",
    "    merge2['pathology'] = merge2['pathology'].replace(['BENIGN_WITHOUT_CALLBACK'],'BENIGN')\n",
    "    #Rename the pathfile into mask for mask dataset before merge\n",
    "    merge2 = merge2.rename(columns={\"Pathfile\" : \"Mask\"})\n",
    "    #Rename the subject ID for mask dataset before mergex\n",
    "    merge2['Subject ID'] = merge2['Subject ID'].str[:-2]\n",
    "    #merge two dataframe into one with pathfile for mask and raw image\n",
    "    merge = pd.merge(merge1, merge2, on=['Subject ID','assessment','pathology'], how='inner')\n",
    "    #keep neccessary column\n",
    "    merge = merge.iloc[:,[0,1,2,4,6]]\n",
    "    #make new column to help sampling\n",
    "    merge['sampling'] = np.where(merge['assessment'] != '4', 1, 2)\n",
    "    merge['type'] = tpe\n",
    "    return merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initiate dataset\n",
    "calc_train_full, calc_train_mask = generate_full_mask('Calc-Training_','calc_case_description_train_set.csv')\n",
    "calc_test_full, calc_test_mask = generate_full_mask('Calc-Test_', 'calc_case_description_test_set.csv')\n",
    "mass_train_full, mass_train_mask = generate_full_mask('Mass-Training_','mass_case_description_train_set.csv')\n",
    "mass_test_full, mass_test_mask = generate_full_mask('Mass-Test_','mass_case_description_test_set.csv')\n",
    "metadata = pd.read_csv('metadata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initiate the variable to get the pathfile for each images\n",
    "names = get_name('CBIS-DDSM')\n",
    "#Make it into dataframe\n",
    "names = pd.DataFrame(names,columns=['Pathfile'])\n",
    "#Make the Match Column to match with File Location in metadata\n",
    "names['Match'] = names['Pathfile'].str[:-12]\n",
    "#Make a new column called Match for joining the metadata with pathfile from names\n",
    "metadata['Match'] = metadata['File Location'].str[2:]\n",
    "#Drop unnecessary column from metadata\n",
    "metadata = metadata.iloc[:,[4,17]]\n",
    "#Merge the new PNG pathfile to metadata\n",
    "result = pd.merge(metadata, names, on='Match', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pathfile for each dataset\n",
    "calc_train = merge_path(calc_train_full,calc_train_mask, 'calc')\n",
    "calc_test = merge_path(calc_test_full,calc_test_mask, 'calc')\n",
    "mass_train = merge_path(mass_train_full,mass_train_mask, 'mass')\n",
    "mass_test = merge_path(mass_test_full,mass_test_mask, 'mass')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat calc and mass dataset\n",
    "training = pd.concat([calc_train, mass_train])\n",
    "test = pd.concat([calc_test, mass_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(data, stratify, size):\n",
    "    rs = 10\n",
    "    # target/input split\n",
    "    Y = data.loc[:,['pathology']]\n",
    "    X = data.drop(['pathology'], axis=1)\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=size, \n",
    "                                                        stratify=data[stratify], \n",
    "                                                        random_state=rs)\n",
    "    return X_test, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the assess 4 dataframe\n",
    "assess_4 = training.loc[training['sampling'] == 2]\n",
    "#sampling the data (50% from sampling data will be based on this data, so the proportion is 0.1*0.5)\n",
    "sample_4, target_4 = split(assess_4, ['pathology','type'], 0.05)\n",
    "#create the assess 1-3 and 4-6\n",
    "assess_r = training.loc[training['sampling'] == 1]\n",
    "#sampling\n",
    "sample_r, target_r = split(assess_r, ['pathology','type'], 0.05)\n",
    "#merge sampling\n",
    "training_sample = pd.concat([sample_r, sample_4])\n",
    "training_target = pd.concat([target_r, target_4])\n",
    "training_target['encode'] = np.where(training_target['pathology'] != 'BENIGN', 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sampling the test dataset\n",
    "assess_4 = test.loc[test['sampling'] == 2]\n",
    "#sampling the data\n",
    "sample_4, target_4 = split(assess_4, ['pathology','type'], 0.08)\n",
    "#create the assess 1-3 and 4-6\n",
    "assess_r = test.loc[test['sampling'] == 1]\n",
    "#sampling\n",
    "sample_r, target_r = split(assess_r, ['pathology','type'], 0.08)\n",
    "#merge sampling\n",
    "test_sample = pd.concat([sample_r, sample_4])\n",
    "test_target = pd.concat([target_r, target_4])\n",
    "test_target['encode'] = np.where(test_target['pathology'] != 'BENIGN', 0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier - ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to load image\n",
    "def load_image(path, target_size, scale_factor):     \n",
    "    im = Image.open(path)\n",
    "    w, h = im.size\n",
    "    im = im.resize((target_size, target_size))\n",
    "    im=np.asarray(im) / scale_factor\n",
    "    return np.asarray(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_full_img = []\n",
    "training_mask_img = []\n",
    "test_full_img = []\n",
    "test_mask_img = []\n",
    "\n",
    "training_full = training_sample['Pathfile']\n",
    "training_mask = training_sample['Mask']\n",
    "test_full = test_sample['Pathfile']\n",
    "test_mask = test_sample['Mask']\n",
    "\n",
    "#iteration to read the image\n",
    "for path in training_full:\n",
    "    load = load_image(path,256,255.0)\n",
    "    training_full_img.append(load)\n",
    "for path in training_mask:\n",
    "    load = load_image(path,256,255.0)\n",
    "    training_mask_img.append(load)\n",
    "for path in test_full:\n",
    "    load = load_image(path,256,255.0)\n",
    "    test_full_img.append(load)\n",
    "for path in test_mask:\n",
    "    load = load_image(path,256,255.0)\n",
    "    test_mask_img.append(load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_full_img = np.reshape(training_full_img, (len(training_full_img),256,256,1))\n",
    "training_mask_img = np.reshape(training_mask_img, (len(training_full_img),256,256,1))\n",
    "test_full_img = np.reshape(test_full_img, (len(test_full_img),256,256,1))\n",
    "test_mask_img = np.reshape(test_mask_img, (len(test_mask_img),256,256,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(57, 256, 256, 1)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pathology = np.asarray(training_target['encode'])\n",
    "test_pathology = np.asarray(test_target['encode'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pathology = keras.utils.to_categorical(train_pathology, 2)\n",
    "test_pathology = keras.utils.to_categorical(test_pathology, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Show image to check if it's load without error\n",
    "#fig = plt.figure(figsize=[10, 10])\n",
    "#for i in range(20):\n",
    "    #ax = fig.add_subplot(4, 5, i + 1)\n",
    "    #ax.imshow(training_full_img[i,:,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"simple_classifier\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "img (InputLayer)             [(None, 256, 256, 1)]     0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 252, 252, 8)       208       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 126, 126, 8)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 122, 122, 16)      3216      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 61, 61, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 57, 57, 32)        12832     \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 103968)            0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                6654016   \n",
      "_________________________________________________________________\n",
      "pathology_out (Dense)        (None, 2)                 130       \n",
      "=================================================================\n",
      "Total params: 6,670,402\n",
      "Trainable params: 6,670,402\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#TRY WITH SIMPLE NETWORK\n",
    "inputs = keras.Input(shape=(256, 256, 1 ), name='img')\n",
    "x = layers.Conv2D(filters=8, kernel_size=(5,5), activation='relu')(inputs)\n",
    "x = layers.MaxPool2D(pool_size=(2, 2))(x)\n",
    "x = layers.Conv2D(filters=16, kernel_size=(5,5), activation='relu')(x)\n",
    "x = layers.MaxPool2D(pool_size=(2, 2))(x)\n",
    "x = layers.Conv2D(filters=32, kernel_size=(5,5), activation='relu')(x)\n",
    "x = layers.Flatten()(x)\n",
    "x1 = layers.Dense(64, activation='relu')(x)\n",
    "pathology = layers.Dense(2, name='pathology_out')(x1)\n",
    "\n",
    "model_cnn = keras.Model(inputs=inputs, outputs=[pathology], name='simple_classifier')\n",
    "model_cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cnn.compile(optimizer=tf.keras.optimizers.Adam(), \n",
    "                  loss='binary_crossentropy', \n",
    "                  metrics=[tf.keras.metrics.Accuracy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "3/3 [==============================] - 3s 873ms/step - loss: 6.2953 - accuracy: 0.0000e+00 - val_loss: 5.9196 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/20\n",
      "3/3 [==============================] - 3s 801ms/step - loss: 6.6388 - accuracy: 0.0000e+00 - val_loss: 5.9196 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/20\n",
      "3/3 [==============================] - 3s 856ms/step - loss: 6.3092 - accuracy: 0.0000e+00 - val_loss: 5.9196 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/20\n",
      "3/3 [==============================] - 3s 828ms/step - loss: 6.2493 - accuracy: 0.0000e+00 - val_loss: 5.9196 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/20\n",
      "3/3 [==============================] - 3s 913ms/step - loss: 6.3092 - accuracy: 0.0000e+00 - val_loss: 5.9196 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/20\n",
      "3/3 [==============================] - 3s 794ms/step - loss: 6.2493 - accuracy: 0.0000e+00 - val_loss: 5.9196 - val_accuracy: 0.0000e+00\n",
      "Epoch 7/20\n",
      "3/3 [==============================] - 3s 813ms/step - loss: 6.3092 - accuracy: 0.0000e+00 - val_loss: 5.9196 - val_accuracy: 0.0000e+00\n",
      "Epoch 8/20\n",
      "3/3 [==============================] - 3s 811ms/step - loss: 6.6088 - accuracy: 0.0000e+00 - val_loss: 5.9196 - val_accuracy: 0.0000e+00\n",
      "Epoch 9/20\n",
      "3/3 [==============================] - 3s 795ms/step - loss: 6.1595 - accuracy: 0.0000e+00 - val_loss: 5.9196 - val_accuracy: 0.0000e+00\n",
      "Epoch 10/20\n",
      "3/3 [==============================] - 3s 822ms/step - loss: 6.2793 - accuracy: 0.0000e+00 - val_loss: 5.9196 - val_accuracy: 0.0000e+00\n",
      "Epoch 11/20\n",
      "3/3 [==============================] - 3s 810ms/step - loss: 6.0996 - accuracy: 0.0000e+00 - val_loss: 5.9196 - val_accuracy: 0.0000e+00\n",
      "Epoch 12/20\n",
      "3/3 [==============================] - 3s 795ms/step - loss: 6.2493 - accuracy: 0.0000e+00 - val_loss: 5.9196 - val_accuracy: 0.0000e+00\n",
      "Epoch 13/20\n",
      "3/3 [==============================] - 3s 813ms/step - loss: 6.6388 - accuracy: 0.0000e+00 - val_loss: 5.9196 - val_accuracy: 0.0000e+00\n",
      "Epoch 14/20\n",
      "3/3 [==============================] - 3s 811ms/step - loss: 6.1295 - accuracy: 0.0000e+00 - val_loss: 5.9196 - val_accuracy: 0.0000e+00\n",
      "Epoch 15/20\n",
      "3/3 [==============================] - 3s 812ms/step - loss: 6.1295 - accuracy: 0.0000e+00 - val_loss: 5.9196 - val_accuracy: 0.0000e+00\n",
      "Epoch 16/20\n",
      "3/3 [==============================] - 3s 788ms/step - loss: 6.3392 - accuracy: 0.0000e+00 - val_loss: 5.9196 - val_accuracy: 0.0000e+00\n",
      "Epoch 17/20\n",
      "3/3 [==============================] - 3s 836ms/step - loss: 6.5189 - accuracy: 0.0000e+00 - val_loss: 5.9196 - val_accuracy: 0.0000e+00\n",
      "Epoch 18/20\n",
      "3/3 [==============================] - 3s 861ms/step - loss: 6.6388 - accuracy: 0.0000e+00 - val_loss: 5.9196 - val_accuracy: 0.0000e+00\n",
      "Epoch 19/20\n",
      "3/3 [==============================] - 3s 880ms/step - loss: 6.2194 - accuracy: 0.0000e+00 - val_loss: 5.9196 - val_accuracy: 0.0000e+00\n",
      "Epoch 20/20\n",
      "3/3 [==============================] - 3s 1s/step - loss: 5.9797 - accuracy: 0.0000e+00 - val_loss: 5.9196 - val_accuracy: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "history = model_cnn.fit(training_full_img,[train_pathology],\n",
    "                    batch_size=64,\n",
    "                    epochs=20,\n",
    "                    validation_data=(test_full_img,[test_pathology]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Classification metrics can't handle a mix of multilabel-indicator and binary targets",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-119-348b7630e029>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_title\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Accuracy'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m \u001b[0meval_models\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_cnn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_full_img\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_pathology\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-119-348b7630e029>\u001b[0m in \u001b[0;36meval_models\u001b[1;34m(model, x_test, y_test, history)\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mfig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0max\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_subplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mcm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[0mc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mConfusionMatrixDisplay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdisplay_labels\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mclassify\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0max\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# extra_args > 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py\u001b[0m in \u001b[0;36mconfusion_matrix\u001b[1;34m(y_true, y_pred, labels, sample_weight, normalize)\u001b[0m\n\u001b[0;32m    294\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    295\u001b[0m     \"\"\"\n\u001b[1;32m--> 296\u001b[1;33m     \u001b[0my_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    297\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0my_type\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"binary\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"multiclass\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    298\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"%s is not supported\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0my_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m     91\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_type\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m         raise ValueError(\"Classification metrics can't handle a mix of {0} \"\n\u001b[1;32m---> 93\u001b[1;33m                          \"and {1} targets\".format(type_true, type_pred))\n\u001b[0m\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m     \u001b[1;31m# We can't have more than one value on y_type => The set is no more needed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Classification metrics can't handle a mix of multilabel-indicator and binary targets"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def eval_models(model, x_test, y_test, history):\n",
    "\n",
    "    predictions_label = model.predict(x_test)\n",
    "    \n",
    "    indexes = tf.argmax(predictions_label, axis=1)\n",
    "    fig = plt.figure(figsize=[10, 10])\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    cm = confusion_matrix(y_test, indexes)\n",
    "    c = ConfusionMatrixDisplay(cm, display_labels= classify)\n",
    "    c.plot(ax = ax)\n",
    "    ax.set_title(' confusion matrix')\n",
    "    \n",
    "    fig = plt.figure(figsize=[15, 15])\n",
    "    ax = fig.add_subplot(3, 1, 1)\n",
    "    ax.plot(history.history['loss'], label = 'Overall Loss')\n",
    "    ax.plot(history.history['_out_loss'], label = ' loss')\n",
    "    ax.legend()\n",
    "    ax.set_title('Training Loss')\n",
    "\n",
    "    ax = fig.add_subplot(3, 1, 2)\n",
    "    ax.plot(history.history['val_loss'], label = 'Overall Loss')\n",
    "    ax.plot(history.history['val_'+'_out_loss'], label = ' loss')\n",
    "    ax.legend()\n",
    "    ax.set_title('Validation Loss')\n",
    "    \n",
    "    ax = fig.add_subplot(3, 1, 3)\n",
    "    ax.plot(history.history['_out_accuracy'], label = ' training accuracy')\n",
    "    ax.plot(history.history['val_'+'_out_accuracy'], label = ' test accuracy')\n",
    "    ax.legend()\n",
    "    ax.set_title('Accuracy')\n",
    "\n",
    "eval_models(model_cnn, test_full_img, test_pathology, history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
